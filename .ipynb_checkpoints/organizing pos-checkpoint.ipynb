{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing parts of speech\n",
    "This notebook is used to identify the parts of speech (pos) for each word spoken by Jerry, George, Kramer, and Elaine. These pos will be be sorted by their types and saved in dataframes corresponding to their respective characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TweetTokenizer, PunktSentenceTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the parsed script\n",
    "script_df = pd.read_pickle(\"Lines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining sentence tokenizer for each character\n",
    "def sent_tokenize(char):\n",
    "    \n",
    "    #pulling all the lines pertaining to the specified character\n",
    "    lines = []\n",
    "    for i in range(len(script_df)):\n",
    "        if script_df.iloc[i][0] == char:\n",
    "            lines.append(script_df.iloc[i][1])\n",
    "    \n",
    "    #training a tokenizer to tokenize the sentences\n",
    "    tokenizer = PunktSentenceTokenizer(lines[0])\n",
    "    \n",
    "    #tokenizing each line and add all the sentences to a list\n",
    "    sent = []\n",
    "    for i in lines:\n",
    "        sent += tokenizer.tokenize(i)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the characters\n",
    "jerry_sent = sent_tokenize(\"JERRY\")\n",
    "elaine_sent = sent_tokenize(\"ELAINE\")\n",
    "george_sent = sent_tokenize(\"GEORGE\")\n",
    "kramer_sent = sent_tokenize(\"KRAMER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tagging each word for each character's lines with its associated part-of-speech\n",
    "def tagging(char_sent):\n",
    "    \n",
    "    #tokenizing the words in each sentence, \n",
    "    #tagging them with pos, \n",
    "    #and adding the result to a list\n",
    "    words_tagged = []\n",
    "    pos = []\n",
    "    for i in char_sent:\n",
    "        words = word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        words_tagged += tagged\n",
    "        \n",
    "    #creating a list of the types of pos used in the lines\n",
    "    for i in range(len(jerry_words_tagged)):\n",
    "        if jerry_words_tagged[i][1] not in pos:\n",
    "            pos.append(jerry_words_tagged[i][1])\n",
    "    return words_tagged, pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organizing the words in the lines based on their type of pos\n",
    "def pos_organizer(tagged,pos):\n",
    "    \n",
    "    pos_org = []\n",
    "    \n",
    "    #creating a list of pos lists\n",
    "    for j in range(len(pos)):\n",
    "        temp = []\n",
    "        \n",
    "        #pulling the words that match a specific tag\n",
    "        for i in range(len(tagged)):\n",
    "            if tagged[i][1] == pos[j]:\n",
    "                temp.append(tagged[i][0])\n",
    "        pos_org.append(temp)\n",
    "    return pos_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating tagged sentences\n",
    "jerry_sent_tagged, pos = tagging(jerry_sent)\n",
    "\n",
    "#organizing pos\n",
    "jerry_pos_org = pos_organizer(jerry_sent_tagged,pos)\n",
    "\n",
    "#creating pos dataframe\n",
    "jerry_pos_df = pd.DataFrame(jerry_pos_org)\n",
    "jerry_pos_df = np.transpose(jerry_pos_df)\n",
    "jerry_pos_df.columns = pos\n",
    "george_sent_tagged, pos = tagging(george_sent)\n",
    "george_pos_org = pos_organizer(george_sent_tagged,pos)\n",
    "george_pos_df = pd.DataFrame(george_pos_org)\n",
    "george_pos_df = np.transpose(george_pos_df)\n",
    "george_pos_df.columns = pos\n",
    "elaine_sent_tagged, pos = tagging(elaine_sent)\n",
    "elaine_pos_org = pos_organizer(elaine_sent_tagged,pos)\n",
    "elaine_pos_df = pd.DataFrame(elaine_pos_org)\n",
    "elaine_pos_df = np.transpose(elaine_pos_df)\n",
    "elaine_pos_df.columns = pos\n",
    "kramer_sent_tagged, pos = tagging(kramer_sent)\n",
    "kramer_pos_org = pos_organizer(kramer_sent_tagged,pos)\n",
    "kramer_pos_df = pd.DataFrame(kramer_pos_org)\n",
    "kramer_pos_df = np.transpose(kramer_pos_df)\n",
    "kramer_pos_df.columns = pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "jerry_pos_df.to_pickle('pos/jerry_pos.csv')\n",
    "elaine_pos_df.to_pickle('pos/elaine_pos.csv')\n",
    "george_pos_df.to_pickle('pos/george_pos.csv')\n",
    "kramer_pos_df.to_pickle('pos/kramer_pos.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
